# Лекция 1 — Невронни модели за класификация (обобщение)

## 1) Невронна мрежа и „невронен елемент“

* Изкуственият неврон е вдъхновен от биологичния и е основен градивен блок на невронните мрежи. 
* **Идея:** вход → линейна трансформация → активационна функция → изход.

### Пример: един неврон за регресия (цена на жилище)

* Вход: площ (x), изход: прогнозна цена (y).
* Използваме **ReLU**, за да гарантираме неотрицателен изход:

$$
\mathrm{ReLU}(x) = \max(0, x)
$$


### Пример: по-сложна мрежа

* Цената зависи от много фактори (площ, спални, локация, пощенски код, пазар и т.н.).
* Мрежата комбинира входовете чрез **скрити единици**, които научават вътрешни представяния и нелинейни зависимости. 

---

## 2) Контролирано обучение: регресия vs класификация

* При **контролирано обучение** имаме означени данни: за всеки вход знаем правилния изход.
* Два основни типа задачи:

  * **Регресия**: непрекъснати стойности (напр. цена).
  * **Класификация**: категория/клас (напр. цифра 0–9). 

---

## 3) Видове невронни мрежи

* **Standard (Feed-forward) NN**: общи задачи с таблични/векторни признаци.
* **CNN**: изображения (улавят пространствени модели).
* **RNN**: последователности (текст, реч, превод) — пазят информация във времето.
* Има и **хибридни** архитектури (напр. автономно шофиране). 

---

## 4) Структурирани vs неструктурирани данни

* **Структурирани**: фиксирани полета (цена, възраст, брой продукти) — лесни за класически модели.
* **Неструктурирани**: изображения, аудио, текст, видео — често изискват по-сложни модели (невронни мрежи). 

---

## 5) Двоична класификация (пример „котка“)

* Цел: моделът да предскаже (y \in {0,1}) (котка/без котка) по вход (x) (вектор от признаци). 
* Изображение (64 \times 64) с 3 канала (RGB) се „разгъва“ във вектор:

$$
n_x = 64 \cdot 64 \cdot 3 = 12288
$$


---

## 6) Нотация за данните

* Обучаващи примери: ((x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)})), където (x^{(i)} \in \mathbb{R}^{n_x}).
* В лекцията е използвана конвенция: **примерите са по колони**:

  * (X \in \mathbb{R}^{n_x \times m}), (X.\text{shape} = (n_x, m))
  * (Y \in \mathbb{R}^{1 \times m}), (Y.\text{shape} = (1, m)) 

---

## 7) Линеен класификатор (интуиция)

* Решаваща граница между класове = **хиперравнина**.
* Знакът на оценката показва класа, а абсолютната стойност — „увереност“. 

---

## 8) Логистична регресия като (най-проста) невронна мрежа

### Модел

* Параметри: (w \in \mathbb{R}^{n_x}), (b \in \mathbb{R})
* Линеен предиктор:

$$
z = w^T x + b
$$

* Прогноза (вероятност):

$$
\hat{y} = \sigma(z) = \sigma(w^T x + b)
$$


### Сигмоидна функция

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$


Свойства:

* (z \gg 0 \Rightarrow \sigma(z) \approx 1)
* (z \ll 0 \Rightarrow \sigma(z) \approx 0)
* (z = 0 \Rightarrow \sigma(z) = 0.5) 

---

## 9) Вероятностен модел и Bernoulli

* За двоична класификация (y) се моделира с **Бернули**:

  * (P(y=1 \mid x) = p), (P(y=0 \mid x)=1-p)
* В логистичната регресия:

$$
p_i = \sigma(w^T x_i + b)
$$


---

## 10) Logit (лог-odds) и връзка с линейния предиктор

* Лог-odds (шансове) са линейни по (x):

$$
\log\frac{P(y=1)}{P(y=0)} = w^T x + b
$$


* **logit** функция:

$$
\mathrm{logit}(p) = \log\frac{p}{1-p}
$$


---

## 11) Функция на загубата и функция на разходите

### Защо не MSE за логистична регресия?

* Квадратичната загуба води до **неизпъкнала** цел (много локални оптимуми). 

### Кръстосана ентропия (log loss) за един пример

$$
\mathcal{L}(\hat{y}, y) = -\Big(y\log(\hat{y}) + (1-y)\log(1-\hat{y})\Big)
$$


Частни случаи:

* Ако (y=1): (\mathcal{L} = -\log(\hat{y}))
* Ако (y=0): (\mathcal{L} = -\log(1-\hat{y})) 

### Разход (средно по (m) примера)

$$
J(w,b) = -\frac{1}{m}\sum_{i=1}^{m}\Big(y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)})\Big)
$$


---

## 12) Оптимизация: MLE и градиентно спускане

* Максимизирането на likelihood е еквивалентно на **минимизиране на отрицателния log-likelihood** → същото като cross-entropy. 
* Градиент (идеята в лекцията):

$$
\nabla J(w) = \frac{1}{m}\sum (f(x) - y),x
$$

* Актуализация:

$$
w := w - \alpha \nabla J(w)
$$


---

## 13) GLM (Обобщени линейни модели)

* GLM свързва (E[Y \mid X]) с линейния предиктор (w^T x) чрез link функция (g(\cdot)):

$$
g(E[Y \mid X]) = w^T x
$$


Примери (от лекцията):

* Линейна регресия: Normal + identity link
* Логистична регресия: Binomial + logit link
* Поасонова регресия: Poisson + (\log) link 

---

## 14) Ключови изводи (резюме)

* Логистичната регресия е **класификатор**, който връща вероятности чрез (\sigma(w^T x + b)).
* Моделът има естествена връзка с Bernoulli и MLE.
* Правилната загуба за двоична класификация е cross-entropy (изпъкнала).
* Оптимизацията се прави с градиентно спускане. 
