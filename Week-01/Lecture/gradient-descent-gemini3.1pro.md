# Обобщение: Градиентно спускане и Логистична регресия

Тази лекция надгражда основите на логистичната регресия, като въвежда методите за оптимизация на параметрите чрез градиентно спускане, изчислителни графи и векторизация в Python.

## 1. Градиентно спускане (Gradient Descent)
* Целта на градиентното спускане е да намери стойностите на теглата (w) и прага (b), които минимизират функцията на разходите J(w,b) до нейния глобален оптимум.
* Алгоритъмът работи итеративно, като актуализира параметрите стъпка по стъпка в посока, обратна на градиента (производната на функцията на разходите).
* Формулата за обновяване на теглата е: w := w - alpha * (dJ / dw), където "alpha" е скоростта на обучение (learning rate).

## 2. Изчислителен граф и Обратно разпространение (Backpropagation)
* Изчислителният граф представя математическите операции като възли и връзки, което улеснява пресмятането на сложни функции на стъпки.
* Обратното разпространение на грешката използва верижното правило (chain rule), за да изчисли градиентите: локалният градиент се умножава по градиента, идващ от следващия възел (upstream gradient).
* Различните математически операции имат специфично поведение при обратното разпространение:
  * Събирането (add gate) работи като "дистрибутор" на градиента.
  * Умножението (mul gate) работи като "превключвател" или мащабира градиента спрямо другия вход.
  * Операцията за максимум (max gate) работи като "рутер", насочвайки градиента само по пътя на максималната стойност.
* Когато една променлива участва в повече от един изчислителен път, нейните градиенти от различните клонове се събират.

## 3. Обучение с множество примери (m примера)
* Обучението на модела с "m" на брой примера се извършва чрез усредняване на загубата (loss) за всички индивидуални примери.
* Основните производни за един пример при логистичната регресия са:
  * dz = a - y.
  * dw = x * dz.
  * db = dz.
* При итерация през всички примери, тези стойности се акумулират и накрая се разделят на "m", за да се получи средният градиент.

## 4. Векторизация с Python (Numpy)
* За да се избегнат бавните "for" цикли в Python, се използва векторизация, която позволява едновременното изчисляване на всички примери чрез матрични операции.
* Векторизирани стъпки за един цикъл на градиентното спускане:
  * Права посока (Forward pass): Z = np.dot(w.T, X) + b.
  * Активация: A = sigma(Z).
  * Обратна посока (Backward pass): dZ = A - Y.
  * Градиенти: dw = (1/m) * X * dZ.T  и db = (1/m) * np.sum(dZ).
* Накрая параметрите се обновяват векторизирано: w = w - alpha * dw и b = b - alpha * db.
