# Обобщение на лекцията: Невронни модели за класификация

Това е структурирано обобщение на лекцията, обхващащо основите на невронните модели и логистичната регресия като основен алгоритъм за класификация.

### 1. Въведение в невронните мрежи и данните
* **Невронен елемент**: Основният градивен блок на изкуствените невронни мрежи, вдъхновен от биологичните неврони. В прости модели той прилага линейна трансформация върху входа, последвана от активационна функция (например ReLU), за да произведе смислен изход.
* **Видове мрежи**: Конволюционните (CNN) се използват основно за изображения. Рекурентните (RNN) обработват последователни данни като текст или реч. Хибридните архитектури се използват за сложни задачи като автономно шофиране.
* **Данни**: Дълбокото самообучение се развива благодарение на голямото количество данни и по-бързите изчисления. Данните могат да бъдат структурирани (с ясно дефинирани полета) или неструктурирани (сурови пиксели, аудио, текст).

### 2. Двоична класификация
* При двоичната класификация целта е да се обучи класификатор, който предвижда дискретен етикет, приемащ стойности като $y \in \{0, 1\}$ (напр. 1 за котка, 0 за липса на котка). 
* Входните данни се представят като вектор на характеристиките $x \in \mathbb{R}^{n_x}$.

### 3. Логистична регресия
* Линейната регресия не е подходяща за класификация, тъй като прогнозите могат да бъдат извън интервала [0, 1]. 
* Затова се използва логистична регресия, която моделира вероятности. 
* Тя използва **сигмоидната функция**, за да преобразува изхода в стойност между 0 и 1:
  $$\sigma(z) = \frac{1}{1 + e^{-z}}$$ 
* Изходът на модела се изчислява чрез формулата:
  $$\hat{y} = \sigma(w^T x + b)$$ 
  *(където $w$ са теглата, а $b$ е прагът)*.

### 4. Функция на разходите (Cost Function)
* За да се обучи моделът, трябва да се минимизира разликата между предсказаните и истинските стойности. 
* Ако се използва стандартната квадратична функция, цената става сложна нелинейна функция с много локални оптимуми. 
* Затова се използва функция на загубата (Cross-Entropy Loss), която е изпъкнала и се базира на метода на максималната вероятност.
* Функцията за загуба за **един пример** е:
  $$\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = -(y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}))$$ 
* Общата **функция на разходите** за целия тренировъчен набор от $m$ примера е:
  $$J(w, b) = -\frac{1}{m}\sum_{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]$$ 

### 5. Оптимизация чрез градиентно спускане
* Градиентното спускане се използва за оптимизиране на параметрите.
* Параметрите се актуализират итеративно с помощта на формулата:
  $$w := w - \alpha \nabla J(w)$$ 
* Скоростта на обучение ($\alpha$) контролира размера на стъпката.

### 6. Обобщени линейни модели (GLM)
* Логистичната регресия принадлежи към Обобщените линейни модели (GLM) и произлиза от разпределението на Бернули в рамките на експоненциалното семейство. 
* Връзката между очакваната стойност и линейния предиктор се осъществява чрез функцията за канонична връзка (функцията logit):
  $$g(p) = \log\left(\frac{p}{1 - p}\right)$$ 
