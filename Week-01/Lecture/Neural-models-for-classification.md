# Обобщение на лекцията: Невронни модели за класификация

[cite_start]Това е структурирано обобщение на лекцията, обхващащо основите на невронните модели и логистичната регресия като основен алгоритъм за класификация[cite: 1, 296].

### 1. Въведение в невронните мрежи и данните
* [cite_start]**Невронен елемент**: Основният градивен блок на изкуствените невронни мрежи, вдъхновен от биологичните неврони[cite: 7]. [cite_start]В прости модели той прилага линейна трансформация върху входа, последвана от активационна функция (например ReLU), за да произведе смислен изход[cite: 12, 20].
* [cite_start]**Видове мрежи**: Конволюционните (CNN) се използват основно за изображения[cite: 75, 76]. [cite_start]Рекурентните (RNN) обработват последователни данни като текст или реч[cite: 77]. [cite_start]Хибридните архитектури се използват за сложни задачи като автономно шофиране[cite: 79].
* [cite_start]**Данни**: Дълбокото самообучение се развива благодарение на голямото количество данни и по-бързите изчисления[cite: 131]. [cite_start]Данните могат да бъдат структурирани (с ясно дефинирани полета) или неструктурирани (сурови пиксели, аудио, текст)[cite: 114, 116].

### 2. Двоична класификация
* [cite_start]При двоичната класификация целта е да се обучи класификатор, който предвижда дискретен етикет, приемащ стойности като $y \in \{0, 1\}$ (напр. 1 за котка, 0 за липса на котка)[cite: 168, 289]. 
* [cite_start]Входните данни се представят като вектор на характеристиките $x \in \mathbb{R}^{n_x}$[cite: 168, 320].

### 3. Логистична регресия
* [cite_start]Линейната регресия не е подходяща за класификация, тъй като прогнозите могат да бъдат извън интервала [0, 1][cite: 390]. 
* [cite_start]Затова се използва логистична регресия, която моделира вероятности[cite: 314]. 
* [cite_start]Тя използва **сигмоидната функция**, за да преобразува изхода в стойност между 0 и 1[cite: 315]:
  [cite_start]$$\sigma(z) = \frac{1}{1 + e^{-z}}$$ [cite: 335]
* Изходът на модела се изчислява чрез формулата:
  [cite_start]$$\hat{y} = \sigma(w^T x + b)$$ [cite: 324]
  [cite_start]*(където $w$ са теглата, а $b$ е прагът)*[cite: 322, 323].

### 4. Функция на разходите (Cost Function)
* [cite_start]За да се обучи моделът, трябва да се минимизира разликата между предсказаните и истинските стойности[cite: 302]. 
* [cite_start]Ако се използва стандартната квадратична функция, цената става сложна нелинейна функция с много локални оптимуми[cite: 488, 489, 490]. 
* [cite_start]Затова се използва функция на загубата (Cross-Entropy Loss), която е изпъкнала и се базира на метода на максималната вероятност[cite: 491, 599].
* Функцията за загуба за **един пример** е:
  [cite_start]$$\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = -(y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}))$$ [cite: 615]
* Общата **функция на разходите** за целия тренировъчен набор от $m$ примера е:
  [cite_start]$$J(w, b) = -\frac{1}{m}\sum_{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]$$ [cite: 636]

### 5. Оптимизация чрез градиентно спускане
* [cite_start]Градиентното спускане се използва за оптимизиране на параметрите[cite: 762].
* Параметрите се актуализират итеративно с помощта на формулата:
  [cite_start]$$w := w - \alpha \nabla J(w)$$ [cite: 737]
* [cite_start]Скоростта на обучение ($\alpha$) контролира размера на стъпката[cite: 738].

### 6. Обобщени линейни модели (GLM)
* [cite_start]Логистичната регресия принадлежи към Обобщените линейни модели (GLM) и произлиза от разпределението на Бернули в рамките на експоненциалното семейство[cite: 692, 712, 757]. 
* [cite_start]Връзката между очакваната стойност и линейния предиктор се осъществява чрез функцията за канонична връзка (функцията logit)[cite: 720]:
  [cite_start]$$g(p) = \log\left(\frac{p}{1 - p}\right)$$ [cite: 721]
