# Обобщение: Градиентно спускане и Логистична регресия

Тази лекция надгражда основите на логистичната регресия, като въвежда методите за оптимизация на параметрите чрез градиентно спускане, изчислителни графи и векторизация в Python.

## 1. Градиентно спускане (Gradient Descent)
* [cite_start]Целта на градиентното спускане е да намери стойностите на теглата (w) и прага (b), които минимизират функцията на разходите J(w,b) до нейния глобален оптимум[cite: 781, 784].
* [cite_start]Алгоритъмът работи итеративно, като актуализира параметрите стъпка по стъпка в посока, обратна на градиента (производната на функцията на разходите)[cite: 793, 794, 795].
* [cite_start]Формулата за обновяване на теглата е: w := w - alpha * (dJ / dw), където "alpha" е скоростта на обучение (learning rate)[cite: 800, 801].

## 2. Изчислителен граф и Обратно разпространение (Backpropagation)
* [cite_start]Изчислителният граф представя математическите операции като възли и връзки, което улеснява пресмятането на сложни функции на стъпки[cite: 811].
* [cite_start]Обратното разпространение на грешката използва верижното правило (chain rule), за да изчисли градиентите: локалният градиент се умножава по градиента, идващ от следващия възел (upstream gradient)[cite: 834, 935, 1219].
* Различните математически операции имат специфично поведение при обратното разпространение:
  * [cite_start]Събирането (add gate) работи като "дистрибутор" на градиента[cite: 1283].
  * [cite_start]Умножението (mul gate) работи като "превключвател" или мащабира градиента спрямо другия вход[cite: 1285].
  * [cite_start]Операцията за максимум (max gate) работи като "рутер", насочвайки градиента само по пътя на максималната стойност[cite: 1284].
* [cite_start]Когато една променлива участва в повече от един изчислителен път, нейните градиенти от различните клонове се събират[cite: 1302, 1303, 1304].

## 3. Обучение с множество примери (m примера)
* [cite_start]Обучението на модела с "m" на брой примера се извършва чрез усредняване на загубата (loss) за всички индивидуални примери[cite: 1367].
* Основните производни за един пример при логистичната регресия са:
  * [cite_start]dz = a - y[cite: 1387].
  * [cite_start]dw = x * dz[cite: 1388, 1389].
  * [cite_start]db = dz[cite: 1390].
* [cite_start]При итерация през всички примери, тези стойности се акумулират и накрая се разделят на "m", за да се получи средният градиент[cite: 1384, 1386, 1391].

## 4. Векторизация с Python (Numpy)
* [cite_start]За да се избегнат бавните "for" цикли в Python, се използва векторизация, която позволява едновременното изчисляване на всички примери чрез матрични операции[cite: 1404, 1431].
* Векторизирани стъпки за един цикъл на градиентното спускане:
  * [cite_start]Права посока (Forward pass): Z = np.dot(w.T, X) + b[cite: 1407, 1439].
  * [cite_start]Активация: A = sigma(Z)[cite: 1410, 1440].
  * [cite_start]Обратна посока (Backward pass): dZ = A - Y[cite: 1420, 1441].
  * [cite_start]Градиенти: dw = (1/m) * X * dZ.T [cite: 1431, 1442] [cite_start]и db = (1/m) * np.sum(dZ)[cite: 1432, 1443].
* [cite_start]Накрая параметрите се обновяват векторизирано: w = w - alpha * dw и b = b - alpha * db[cite: 1444, 1445].
